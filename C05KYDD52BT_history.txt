1717829657.987279: <@U077KEARYBT> has joined the channel
1717788490.682159: Hi <@UMZBGNHAL> and <@URXM7FUF3> - I received an update from Dev.

My recommendation for this is that we present the killswitch next week to you, schedule a re-deploy  of autoscaler, and then schedule a test with Perifery on that call to ensure that we can diagnose the loop further as that is the only way we are going to be able to conclude and resolve this issue, by finding it again with a smaller dataset with our dev team on the call. Based on our findings, the only thing we can say conclusively say is the following:

The cause of the panic is due to the large volume of files sent via saved search rather than the small 20 or so asset subset we requested (as you both im sure already know). The bug itself would have and should have been a minor bug ticket and not a full RCR, as this product is utilized by many customers and SF is the only client for which this has happened, therefore its likely related to setting in their environment rather than the Perifery Tool itself, but we have no way of further diagnosing at this time without a test.

Additionally, the dev teams findings were as follows thus far, and they do not believe they will get any more information without receiving either direct access to the clients environment (which will not happen) or a live test on a small subset of data to determine the cause of the bug. They are not willing to make up a full RCR without this test as they have no way of knowing if their guesses are accurate or not.

 *Dev response*: Autoscaler could not get mediainfo for all files and then triggered built-in iconik transcode. We are guessing you had issues with permission on *prod-or-sfmam-s3-iconik-originals* bucket. Make sure the `bucketName` terraform variable is pointing to *prod-or-sfmam-s3-iconik-originals* and autoscaler and the bucket are in the same AWS region and test again.
1717542865.410679: hey Jeff!

I apologize I thought you were included on the email to cameron,

We confirmed that the subclip metadata, step 3 /4 is ready, but we ran into an issue on step 2 with collections and carlos has sent in a ticket saturday morning. The team is now in QA on this, hoping to get it imported tomorrow based on it being in QA today
1717542610.837039: <@U055K2ES0CW> do you have any LAC updates from the weekend?
1717202545.936099: Hey Jeff and team, we are now proceeding with the chargers step 2 importing the collection metadata
1717027754.565939: For SF, let me know if you have a moment to chat…I may have another idea about what may have caused the loop--if your devs haven’t found the RC yet.
1717027708.720189: Good stuff.  Thanks for the update on the Chargers.
1717022319.525149: <@UMZBGNHAL> step 1 of the chargers from the list we discussed will be completed within the next two hours, we are proceeding to begin the next step early tomorrow .
1717022265.585669: <@UMZBGNHAL> I received another update on SF that I sent you over <@URXM7FUF3>
1717003271.041759: 
1717003260.631169: Hi <@U055K2ES0CW> another question as well -- the Orlando Magic are apparently using your archive script, and when they double-checked that it was working, they found that only 3 of 96k assets in the collection they fed it had been properly archived:

They are not sure how to check if other assets are still in a queue other than the ‘jobs’ section, which shows 0 waiting.
1717003033.681379: Thanks, this isn't urgent, just whenever he has a moment!
1717002993.878549: <@U037S3RRK34> I sent this to carlos he will get you a response as soon as he is available
1717002738.863909: <@U02E7HVAVM0> has joined the channel
1717000233.195859: <@U02EYQF4QH4> what unit should the file size column be in the metadata CSV? Bytes?
1716952867.992109: Kk SF is starting to get antsy 

Appreciate y’all putting in the effort 
1716946476.777789: Found*
1716946473.125849: I also let them know what Jeff had foind
1716946452.560639: Hey Addison, Its still priority for our devs, Ill check in now but I know they were not seeing any issue yet with the logs that Salesforce sent over, let me see if they have any followup
1716946374.443519: <@U055K2ES0CW> <@U02EYQF4QH4> need a RCA for SF gents

Can you please make this a priority 
1716928164.920609: Good to hear.  Looking forward to it.
1716928137.965309: hey Jeff, its still processing! its working so far! I will give you an estimate later today
1716928087.134319: <@U055K2ES0CW> just wanted to check-in and see how the Chargers work went/is going from this weekend?
1716327580.312239: hey Addison, I just met with the dev. They have finished evaluating the logs sent over and have requested the following since we do not have access to AWS (or more specifically they requested Access to AWS preferably.

They are requesting access to the following lambdas logs:
`s3newfileevent`, `iconikProcessProxy`, `jobs`
1716327176.539839: <@U055K2ES0CW> <@U02EYQF4QH4>  need a RCA plz
1715988282.463199: its all done now
1715985852.180939: Never mind.  Looks like its still ticking down…783
1715985815.014149: Looks like we’re stuck at 793 jobs “Waiting”
1715985092.693869: excellent, just an update, the developers will be resuming work Sunday on this and I am very hopeful to find out more to give you guys on Monday after my dev meeting at 8 AM
1715984950.351859: 1164
1715982465.395569: We’re down to~3,600 “Waiting”
1715982412.776379: Great!  Thanks.
1715981791.910999: Thanks Carlos!
1715981773.117299: <@UMZBGNHAL> <@U055K2ES0CW> Started the Abort Script, there are 4k or so jobs and its updating each one individually so just give it a moment but its going pretty fast
1715967920.060519: <@U055K2ES0CW> <@U02EYQF4QH4> specifically to the question about removing the SF iconik jobs in the Waiting status, is this something you are handling?
1715963618.988159: yup this is required before we start the autoscaler again from SF
1715963240.970939: <@U055K2ES0CW> <@U02EYQF4QH4> I need a RCA for SF plz
1715903846.608539: Yeah I think to avoid issues with Iconik and transcoding it’s best to remove those through some script. 
1715903756.730289: Gotcha. We have a working session scheduled with Brian for tomorrow at 3pm PST so I’ll make sure we take care of all that on the call. 

What about clearing all the waiting jobs out of iconik?
1715903598.585449: That should be everything to stop it from going through autoscaler 
1715903587.626699: <@UMZBGNHAL> we need to remove all the lambda functions, we need to remove the SNS Queue and in the Originals bucket we need to remove the Notification for On Create of objects
1715903522.703009: Per the screenshots from earlier it appears new ingests are still getting routed through at least one of the lambdas.
1715903485.937199: <@U02EYQF4QH4>
1715903449.618649: In the meantime can we figure out what else I need to remove/disable in AWS to ensure no more jobs get routed to the autoscaler services?
1715903379.664669: hey <@URXM7FUF3> and <@UMZBGNHAL> our dev team is working on it, but we have not yet received an update on what was the cause and potential fix. I will have another update before tomorrow afternoon.
1715889608.887359: <@U055K2ES0CW> really appreciate your effort here. Please provide <@UMZBGNHAL> and I and end of day update on latest findings
1715879700.084749: For reference, here are some screenshots related to my write-up (above):
1715879001.443049: Thanks, Jeff. <@U055K2ES0CW> plz make this a top priority for Salesforce as I know you are
1715878974.215239: I align with your thinking on this Jeff, any concerns on disabling autoscaler that im not aware of carlos
1715878703.706209: <@U055K2ES0CW> <@U02EYQF4QH4> here are some updates from our end on the SF Auto-Scaler…
1. This morning, with Jeffrey at SF, I tested re-enabling the default Iconik Cloud Transcoder on the Originals storage to allow new ingests to transcode properly, but that appears to not be working as intended.
2. As part of this test, we noticed on the iconik Jobs page that the Auto-Scaler is still trying to run on new ingests to the Originals bucket, so we are seeing two transcode jobs on an asset.
3. Based on that finding, I have reverted to disabling the default Iconik Cloud Transcoders on the Originals storage.
4. We then ran the default (iconik) Re-Transcode, which as you recall should run using the default Iconik Cloud Transcoder (despite no transcoder being defined on the Storage)…however, this also appears to not be working.
5. I believed that the Waiting queue in iconik is preventing new Transcode jobs from running.  However, even after bumping up the priority of our test ingests to “10” they are still stuck in a “Waiting” state.
My recommendations:
1. We clear out all “Waiting” transcode jobs from their system to hopefully allow new ingests to be received and handled properly.
2. We get back on the phone with Brian at SF and completely remove/disable the Auto-Scaler so that we can ensure that new ingests and transcoded properly via the default transcode settings.
Thoughts?
1715815197.911959: <@U02EYQF4QH4> ^^
1715815141.875139: <https://imtglobalinc.zoom.us/j/97184045737?pwd=UVR0bk1YK05pbGw0OEFQSHJBckJ5QT09>
1715815101.364029: please add me as well
1715815093.123859: ok <@UMZBGNHAL> can you add him to the call
1715815092.233379: Send me the meeting link 
1715815081.530389: Yup
1715815075.676189: getting a hold of him now
1715815028.835509: <@U02EYQF4QH4> <@U055K2ES0CW> in AWS Console, we are not seeing any activity in the MediaConvert queue in us-west-2.
1715814697.380079: <@U055K2ES0CW> can you track down Carlos for me plz. on the phone with SF
1715814645.301149: <@U02EYQF4QH4> are you available? we're on the phone with the SF contact needed
1715813231.938799: Yeah, once we stop the bleeding here, we should review/discuss safeguards and how to safely test and rollout these powerful workflows.
1715813187.024939: Copy that
1715812800.464559: per Jason, flushing the SNS queue or killing the lambda would be the only option, carlos and I just got off the phone with him
1715812570.098099: shoot
1715812532.554419: Brian is MIA at the moment, they are trying to hunt him down--or anyone from his team.
1715812519.478109: so we can hop on a call and take care of this asap
1715812507.374099: we are in to get in and remove the lambda - but we need brian do to so
1715812503.699979: <@U037S3RRK34> ^^
1715812499.116169: Ok.
1715812440.330809: the custom script will not stop it, our autoscaler is not dependent on that, do not waste your time on that
1715812403.809449: need a kill switch <@U055K2ES0CW> nothing we can do rn, correct?
1715812368.232959: once they get it to stop and re-deploy, we go and test one asset, see if it works as expected, then we take the asset that is looping and test that next.
1715812337.322979: Josh is currently working on a custom script to kill all jobs in iconik, but we’re not sure if that will work in killing the SQS queue.  <@U02EYQF4QH4> can you speak to whether killing the iconik job will removre it from the queue?
1715812275.880659: Once we get this stopped we can dig into the logs.
1715812265.772219: <@U055K2ES0CW> we are still working on killing the SQS…SF is trying to locate the resource who has access.
1715812236.646869: <@URXM7FUF3> <@UMZBGNHAL> can we get the logs asap
1715811974.470109: <@U055K2ES0CW> can you hop on this asap? This is a big deal rn
1715810312.429999: No idea, we are going to need logs to have the team look into it
1715810286.980819: (screenshot above)
1715810279.250989: <@U02EYQF4QH4> any idea why it’s looping?
1715808851.150249: Just so you know carlos will be out after 4:30
1715808844.611949: no problem Jeff!
1715808838.275119: I’ll keep y’all posted.  Thanks for the quick reply
1715808819.188369: ok
1715808801.101449: I mean if you can shut the lamnda down that instantly stops it, we can just have him redeploy this again
1715808780.009099: Is that the right order of operations?
1715808772.547949: We’ll start with SQS, then SNS, then lambda?
1715808763.923609: We’re trying to get Brian on the call so we can get this done.
1715808753.534469: maybe remove the lambda? Im not sure
1715808749.683139: ok
1715808742.720129: yeah flush the queue and the SNS notification if possible
1715808731.100879: ok thanks
1715808723.113539: carlos is looking now
1715808713.198309: Yep…which piece do we shutdown?  Do we need to flush the queue somehow?
1715808682.106839: per Carlos: shut it down on the AWS side, there is not much else we can do to kill it
1715808615.931189: I have a slack call with the customer happening now.  Can switch over to a Zoom if you prefer?
1715808601.827479: yikes
1715808595.908659: Full disclsosure, we told them to run this on 10-25 files to test/validate re-transcode, but they accidentally ran it against the entire saved search :grimacing:
1715808587.771689: I let carlos know
1715808552.898289: I’m thinking we are going to need to kill the queue on the AWS side to stop this runaway process.
1715808525.230769: 
1715808514.411489: <@U02EYQF4QH4> <@U055K2ES0CW> we have a SEV-1 at Salesforce…they accidentally ran Re-Transcode on over 600K assets by accident and it appears they are looping through the transcode step:
1715792985.799169: Will do <@U02EYQF4QH4>, we need to update our import tool to be case-sensitive to these values. As soon as that's done I'll be able to finish this import.
1715791841.645069: <@U037S3RRK34> please let me know when they metadata fields for chargers has been updated
1715718721.369459: <@U055K2ES0CW> are you able to update the above questions from Jeff and Josh?
1715631769.467599: <@U055K2ES0CW> or <@U02EYQF4QH4> per our call with SF last week, can you confirm that when it comes to the Auto-Scaler “Re-Transcode” Custom Action, that this custom action will function properly, regardless of the storage it runs on?  Meaning, if I had a saved search of 300K assets that needed to be “retranscoded” and those Originals reside in an S3  storage other than what is specified in the Auto-Scaler config, that the Auto-Scaler will be able to process those transcodes as expected?
1715621014.890479: <@U02EYQF4QH4> is the CSV import tool case-sensitive to dropdown values? Some of these dropdown values only vary by case; if only one of them is added, will assets that are tagged with the other variation still be tagged after import?
1715383423.772399: Thank you for getting these over so quickly!
1715383412.983989: Yes, these look great! I'm still waiting to hear back about our import tool, so it's looking like this might be something that I tackle first thing Monday morning
1715382825.230789: <@U037S3RRK34> Tell me if this is what you are expecting
1715382524.524109: Yes, please use ‘Label’ and ‘Value’
1715382360.562439: <@U037S3RRK34> I am almost done but I forgot what the headers need to be for the CSV, are they `label` and `value`?
1715376332.544889: <@U055K2ES0CW> I forgot to ask on our LAC call, did you want us to do the full iconik reindex or are you and Carlos going to handle it?  If so, when should we trigger it relative to all the other tasks we discussed?
1715280682.819069: I’m asking Marina to get it over to you 
1715280623.652689: <@U5N6W046L> can we ask Todd for a screenshot of the reach search RQL from VDB and screen grabs of the reach search interface so that we can estimate how long it will take to match that search as much as possible with our potential tool we may end up selling to them through you guys
1715124645.117799: Ok. We can postpone that until the migration.
1715124624.195539: ah yes that one needs to be created manually as we still haven't automated it.
1715124579.323419: Yes I think it's safe to cancel until mid June. Thanks.
1715124559.161779: Thanks Carlos. I'll review. Also, if not done already please add a field to capture the "Categories" from reach.
1715124428.327709: Thanks <@U02EYQF4QH4> <@UJ1N2C67Q> when you get a chance can you please review? if we are good we can cancel our sync until we are ready to do the migration mid-June
1715124385.017019: Note: `re_id` is added by us to keep track of the reach asset ID when migrating
1715124358.941579: <@UJ1N2C67Q> <@U5N6W046L> I added the metadata fields to the BeachBody Iconik, I noticed some fields that were added due to not being in the exclusion list that you might not want on there. I am attaching a result of the output so you can verify the fields.
1714769979.011789: <@U055K2ES0CW> <@U02EYQF4QH4> Chargers Collections have now been restored to their original location.
1714759816.998979: Jeff* hoping your vacation was awesome! I am sending this to the team
1714759800.031399: thanks JEff
1714759794.914559: Here are some links to failures they sent me via Jobs&gt;History:
<https://app.iconik.io/admin/jobs/history/f368417a-0966-11ef-8674-ee43d520a164|MP4>
<https://app.iconik.io/admin/jobs/history/51458726-0967-11ef-9a66-dee974e07015|WAV>
<https://app.iconik.io/admin/jobs/history/1f2822ee-0967-11ef-9ec0-fa131543b2c2|PSD>
<https://app.iconik.io/admin/jobs/history/159c840a-0966-11ef-bc62-b2ca20d61b5f|MOV>
1714759770.100179: <@U055K2ES0CW> th Auto-Scaler seems to be working fairly well at Salesforce.  However, they’ve encountered a number of errors, which appear to auto-recover and complete. The errors are related to MediaInfo.  Perhaps its a race condition where the transcode runs BEFORE the mediainfo extraction has time to complete?
1714182467.096759: Thanks <@U02EYQF4QH4>. This regex pattern is the correct syntax according to iconik though. What do you recommend?
1714171427.788329: ok I am passing this along
1714170771.511999: <@U5N6W046L> <@UMZBGNHAL> The ignore patterns in Iconik are used by Autoscale as we set up in the <http://variables.tf|variables.tf> file and the problem is that the Ignore pattern `re:/(?i)(.*)(\/)(.*)(Adobe Premiere Pro Auto-Save)(.*)/` and the problem is that javascript doesn't support this portion `?i` of the regular expression and is causing autoscale to fail due to invalid regular expression.
1714170239.547659: jeffrey just asked this: Hey everyone. I have a question regarding the exclusion code that held up the auto-scaler deployment today. Why is that code causing an error when there is no scan enabled on the Originals bucket? We only have a scan on the LL ISG and the Legacy ISG where that exclusion is in play.

Carlos will reply here
1713802963.904549: ok thanks, please let us know when you have that update
1713802850.031209: hi <@U5N6W046L> carlos is evaluating today, but i finished the import process by 10 am saturday, now he needs to make sure it all went smooothly
1713802788.864609: GM <@U055K2ES0CW> wanted to check in to see how the Chargers are going?
1713555307.509859: Also, Brian is strongly requesting an EOD update on the metadata migration regardless of status.
1713555221.510679: <@U055K2ES0CW> when is tranche 2 going to run for LA Charges?  They are requesting that you run the tranche (CSV) that contains the NEWEST assets next.
1713542365.589059: Copy that.  Thanks.
1713542296.433819: spoke with Dev, when they told me it was complete, they meant they completed 1 CSV file successfully, there are 3. I am working now to make sure all 3 are done in short order
1713542025.714199: Regarding my last screenshot…that appears to have been a UI bug…I think…It’s no longer there:
1713541999.727929: im looking to hop on a call with them with carlos when he clocks in but no
1713541958.984529: Ok, thanks.  Is there any way we can have a joint discussion with your Dev team?  Seems like we keep getting stuck in a very costly loop.
1713541898.133339: going back to the dev team now
1713541892.693279: I’m also seeing this in the Metadata Fields List:
1713541800.017089: <https://app.iconik.io/savedsearch/a56f9dee-f6d6-11ee-9a6e-56c5a77aeaa3>
1713541766.458499: <@U055K2ES0CW> I’m only seeing 106K assets migrated out of 227K.
1713539965.413699: <@UMZBGNHAL> we have now completed the migration of the data, we will be evaluating today to ensure accuracy.
1713484418.794039: <@U02EYQF4QH4> for Salesforce’s WIN Auto-Scaler deploy, the <http://variables.tf|variables.tf> doc has a section for “targetCollectionId”…can you clarify what this is intended for?  If we are tying this to a storage, then why would I need to specify a collection as well?
1713478003.670939: Just to clarify, while the RE Categories are NOT needed at LA Chargers, they ARE needed for VDBP.
1713477054.420689: Ah you’re right, I’m sorry about that, I checked the csv files for VDB and I see that at the time we didn’t change the name from re_categories to that one as we didn’t have the field name mappings yet and I think it was a migration oversight to connect these fields. We developed the field name mappings for chargers and we can apply it to VDB in the next migration. 
1713473027.489619: <@U02EYQF4QH4> for AFV (VDBP) there is a field already in iconik (with values) that contains the RE categories… `RECategories`
1713473020.316679: <@U05TWNVCHJT> has joined the channel
1713473018.501959: <@U05TWNVCHJT> for tracking
1713473007.558459: we were just spot checking on our call with iconik and Bayard is now very concerned that there is a lot of metadata missing and wants to review
1713472751.242509: do we know why those values aren't there?
1713472709.853469: &amp; it looks like the reach engine id is only applied to 4300? is that accurate? <@U02EYQF4QH4>
1713472369.398689: ok
1713472368.216099: so that will be done in the next round that we do?
1713472363.972009: I will be verifying it by our next migration
1713472334.578579: It seems that the metadata field for categories was not created in Iconik, but we do have the metadata extracted, it just didn’t find a field to apply them to. That must have been an oversight in the field creation. It will need to be a multi drop down called re_categories
1713472168.193779: I know we had discussed but we are on with iconik and not seeing them
1713472148.565809: <@U055K2ES0CW> to confirm- the categories are coming over in the metadata migration for afv?
1713471304.757429: thanks Carlos I will work with Josh and Jet
1713470885.305239: <@U5N6W046L> <@U037S3RRK34> I have updated the drop downs for DDM and checked them, please verify them and let me know.
1713459179.238859: got it, thank you
1713459145.715459: we are discussing all clients now with  the dev team to get clarification on each then will send update emails
1713459027.073019: <@U055K2ES0CW> <@U02EYQF4QH4> any news on the Chargers? and what the issue is? I didn't see an update to the customer either?
1713393663.597819: I believe yes there has been about 10% of the metadata imported, confirming what the issue is and I will summarize
1713393277.071849: I can confirm that I see 36K assets have RE metadata now.  <@U02EYQF4QH4> <@U055K2ES0CW> can you summarize the issue you are seeing?
1713393195.439859: <@U055K2ES0CW> so has nothing been imported as of yet?
1713393100.994789: <@UMZBGNHAL> ^^
1713393037.244389: <@U5N6W046L> <@U7CASSE6B>  Carlos just informed me that we are running into issues with csv migration of chargers and need to pause the migration again to fix the errors on the tool side. we will keep the IMT team updated on this and we apologize for the delay. We realize this is critical and the chargers are getting frustrated and are doing everything to get it up and running.
1713377071.255639: ok thanks
1713377025.994409: Sorry for the delay. I am starting it now. The old collections have now been removed, give it a few minutes to see the new stuff appearing in Iconik
1713373100.084529: <@U02EYQF4QH4> any news on soon?
1713370189.378809: can you define soon? The Chargers are beginning to get frustrated and we need to set the correct expectations
1713370026.176359: <@U5N6W046L> <@UMZBGNHAL> the data took longer to extract than expected. I left it running yesterday past EOD and I will be starting the migration soon today. 
1713369915.717809: According to my saved search, no progress was made on the Chargers…as these are the original 27K that were already migrated:
1713368906.042369: <@U02EYQF4QH4> <@U055K2ES0CW> how are we looking on chargers? 
1713305015.891549: <@U02EYQF4QH4> are you able to join this call with Josh and I on DDM?
1713303898.738079: <@U055K2ES0CW> I don't want to lose time, I am meeting with Carlos at 3 about DDM and can discuss with him there and get at least 1 started
1713298486.031259: Checking
1713296402.137019: <@U055K2ES0CW> ^^
1713295957.645259: and can then have vdbp done in time for a call on thursday?
1713295924.045449: if we start with Chargers we can get that done in 24hrs yes?
1713294782.396759: <@U5N6W046L> we have received word from the dev team that updates have been made and pass QA and we should be good to proceed. My question for you is who takes priority right now? LA Chargers, or VDB? We can only do one at a time unfortunately. Just a heads up, VDB needs the collections created and then metadata added to them. LA chargers we are restarting the whole migration as there were some issues we identified that have now been fixed that needs to be reapplied to the previous 27k assets completed, also because we need to create collections again for those assets.
1713200943.334379: 
1713200939.812759: but I see the specifics, so what more is needed?
1713200860.359929: I dont see an email from Nick?
1713200657.011139: Yes but I did not see any specifics. Like nicks email mentioned we need further clarification. That being said I will look into  it more now to see what is happening. 
1713200510.228749: <@U02EYQF4QH4> you saw the email regarding VDBP?
1713200499.164899: ok and as soon as you have something please pass it along, I will let them know that it is still being worked on and we are hopeful for an end of day update
1713200335.673209: <@U055K2ES0CW> <@U5N6W046L> unfortunately the dev team is still working on it. We will have an update hopefully later today.
1713200284.699319: Checking
1713200027.294399: <@U02EYQF4QH4> <@U055K2ES0CW> do we have an update on the Chargers?  The client is asking
1712941924.241769: thx
1712941491.414849: yes
1712941474.028749: <@U055K2ES0CW> can you please locate?
1712941398.449929: <@U02EYQF4QH4> are you joining this call?
1712874854.385579: 
1712874762.694679: <@U02EYQF4QH4> CSV is looking good.  Customer reviewed it.  They are now requesting that we send over the rest of the CSV data so that they can review it--and also so that they can use it in the meantime to search for metadata as they await the import.  Can you send the rest?
1712873578.736989: We still need to a response as to why we are getting the errors when doing this migration so we need to hold off at least another day to see what is happening from the dev side.
1712873529.307559: Ok, so does this mean we need to wait to run the import?  I’m having the Chargers QC the CSV you sent over. My spot checking so far is looking good
1712873493.790139: Lets not clear it out so that our dev team can take a look and check them
1712873441.528659: Ok.  Did you want to clear them out or do you need me to?
1712873246.644989: okay that should definitely NOT be happening. Unfortunately this is an issue on the CSV migration side and I need to open a dev ticket on our end to investigate. Fortunately these collections can just be removed without removing the assets and we can recreate them easily. It should not affect the migration of metadata
1712872996.057859: While I review, can you please take a look at the collections?  Looks like duplicate collections are being created in the “Reach Engine” parent collection
1712872920.846539: 
1712872899.862589: <@UMZBGNHAL> please verify that this data is correct
1712871483.772869: <@UMZBGNHAL> I am regenerating them now. I can provide the first 100k in about 10min.
1712868355.507799: I had to add additional logic to make this dynamic in the future so we don’t encounter this problem again so I’m still verifying this doesn’t affect any other exports
1712868260.642919: <@U02EYQF4QH4> checking in.  How we looking?
1712862463.452799: <https://imtglobalinc.zoom.us/j/99719039999?pwd=b0tadUp5TmRhN2RkUnNGeGljWDl0QT09>
1712862409.088849: Ah ok, standby.  Going to spin up a zoom bridge
1712862340.138039: I can’t join because I am not a member of the channel 
1712862270.858779: <https://app.slack.com/huddle/T64E9LCG0/D06T279LCUB>
1712862141.155749: Ok great.  2 minutes
1712862134.766019: Yeah
1712862117.942859: Can you join a call with the customer to review?
1712861606.707259: When you say a ton, what amount are we talking about here? The first 28k or so of these were migrated.
1712861438.765439: Sounds good.  Yea we need to get to the bottom of it, but it looks like there is a ton of discrepancies with metdata not being applied to the correct assets.
1712860499.599929: <@UMZBGNHAL> can I send you a generated CSV with metadata and you tell me if this data is inaccurate? This is the first time we’ve had this issue so we definitely want to see the source of it. We don’t have access to reach engine but I’m going off what the database is showing me. 
1712856034.887049: <@U02EYQF4QH4>
1712856030.661249: Ok sounds good.  And can you confirm that you are aware of the metadata inaccuracies?
1712855998.911049: We have stopped the migration and are working with dev to fix this, when we proceed all metadata that has been migrated will be overwritten
1712855944.762449: <@U055K2ES0CW> looks like the Chargers are reporting metadata is inaccurate.  Can we chat?
1712853704.592629: we tested it again this morning on my PC and we had to send a ticket to dev as the CSV tools seems to not be working as expected. I believe its tied to formats and dev is looking into it asap
1712843626.137819: <@U055K2ES0CW> I thought you were going to do the metadata migration piece?  Can you please provide an update on what Carlos mentioned?  Customer expected all the metadata to be in the system by the start of business today.
1712840406.786299: <@UMZBGNHAL> I started the migration last night and I ran into some sort of issue at about 28k files. It seems that there is some issue with our AWS service or it could be my machine, but I have sent the files over to <@U055K2ES0CW> so that he can attempt it on his end as he has done all of them an nothing like this has occurred. Note this is nothing to do with the CSV data itself but more so related to the csv migration tool or my network. Sorry for the delays.
1712788870.548989: Please let me know when you start running it so I can monitor on my end (via a saved search)
1712788832.686729: Copy that
1712788762.756069: Also by EOD is really more that it’s going to run into the night because it won’t be done in the next hour or so but definitely when they log on tomorrow
1712788704.464829: Ah ok.  Great
1712788681.444599: second is for errors
1712788671.889519: i believe the first run is all metadata
1712788651.441119: Gotcha.  How many are in the first run?  Chargers expected most/all of the asset metadata to be complete by EOD today.
1712788561.416469: and finishing the first run
1712788556.168519: i will be starting it today
1712788552.735829: carlos literally is finishing the extraction now
1712788478.020349: <@U02EYQF4QH4> <@U055K2ES0CW> do you have a status update on the Chargers?  Looks like the migration hasn’t started yet.
1712785743.241459: ok thanks
1712785175.584779: It can be update in the future by redeploying
1712785117.543749: can a new app token be created if needed if a new service user is created?
1712785051.003079: I will just be creating an app token under for the user 
1712783667.097989: <@U02EYQF4QH4> do you need that service account user or do you need an app token setup under that user?
1712783625.115589: Gotcha
1712783010.830709: Yup, my only concern is that if that account is ever removed the tool will stop working but if that’s not a concern anytime soon then I can use that one. 
1712782952.781729: The only DDM Iconik service account currently in the system was deployed for another system/service, but may be able to be leveraged for the CSV use case as well.
1712782701.273579: <@U025KD7N9QB> has joined the channel
1712782697.862429: Josh is out until Monday, going to tag in <@U025KD7N9QB> as he might know the answer
1712782655.299539: <@U037S3RRK34> <@U5N6W046L> For DDM what user account do I use to deploy CSV migration, generally speaking we have an Iconik Admin or service Account that is used but I wanted to double check for this client.
1712780056.375309: <@UMZBGNHAL> I am generating the final CSV now to begin the migration. This extraction should take approximately 40min or so. Then we can begin migration right after.
1712767423.369699: checking with <@U02EYQF4QH4>
1712767377.404309: <@U055K2ES0CW> do we have an estimated start time for the LAC Metadata migration today?  The Chargers are asking.
1712687289.954059: <@U055K2ES0CW> just checking in on the LAC metadata migration. How’s it looking?  Also, I wanted to confirm that you are mapping to the correct Reach Engine Asset ID field into iconik:
• CORRECT FIELD:  lac_re_assetid
• INCORRECT FIELD:  re_id
1712596165.233739: Sounds good.  I defer to you on which small sample you want to run, but 2024 probably has the fewest number of assets.
1712595576.538589: <@UMZBGNHAL> we should be ready to migrate all the data we need but I will need to run a small sample first. 
1712592025.166989: so i have not heard from him yet
1712592020.258399: i believe so, but carlos doesnt clock in until around now
1712591920.117079: <@U055K2ES0CW> good mornin!  Just checking to see if we are still on track to begin the metadata migration today?
1712336271.321109: nope not yet!
1712334067.477099: <@U055K2ES0CW> just curious if you have any update on skipping proxies?
1712272172.383869: <@UMZBGNHAL> I am not sure but includng <@U02EYQF4QH4> so that we can all find out
1712271051.614719: <@U055K2ES0CW> quick question from the SF team…for the AutoScaler, if a new S3 object is added to the S3 bucket that the Autoscaler workflow is “watching,” but that object is associated with an asset record that already has a proxy, will it retranscode or skip the proxy creation?
1712185653.073609: Thanks!
1712184195.379039: <@U055K2ES0CW> and <@U02EYQF4QH4> Todd is on it and will send you the details before EOD today.
1712182213.187459: <@U7BG52ZAM> I don’t believe it came up during our call the other day (so you’re all good there!), but we did discuss it on a sync call I had with WIN earlier today…Not sure if you saw the slack message I sent you, but Carlos is wondering if you can point him to where he is able to locate this collection metadata within the RE DB schema.
1712179907.561139: I have a sinking feeling I had promised to send that during that LA Chargers meeting I attended unplanned, and promptly forgot all about it once the meeting ended and I returned to my AFV work.
1712179528.363199: hey guys, if you could get us information about the collections metadata sooner than later in regards to chargers <@U7BG52ZAM> <@UMZBGNHAL>. We are just looking for more clarity from the meeting today!
1712179318.908329: <@UMZBGNHAL> here are the variables for Autoscaler for salesforce, you need to fill in the `default` parameter. If you have any questions or need clarification please let me know.
1711640977.948079: Yes
1711639786.319579: <@U02EYQF4QH4> great!  Thanks again for getting this done.  Yeah do you have time now?
1711639571.000969: I have some time to meet really quick before your meeting if you are available
1711639556.506809: <@UMZBGNHAL> quick update, I got the metadata on those 69 files present in March, the only thing we couldn't migrate over due to a bug that is being fixed by our dev team right now is the Mezzanine association, we can run the migration again and it will work once the feature has been fixed. Seems there was a bug introduced when doing the Versions update for VDB.
1711486701.685779: Can we please being the csv import tomorrow?
1711486674.882899: We will just take care of the DB extraction and get that sent over by EOD today.
1711486648.784689: Not complete…but started
1711486201.688539: yeah i was not under the impression that we had to have this metadata completed by thursday, im not sure that is possible , lets plan to talk tomorrow
1711485988.689069: <@U5N6W046L> please ask <@U055K2ES0CW> to see what works for him, but generally speaking probably best sometime after the VDB
1711485712.034579: I can move it tomorrow, just let me know what works best for everyone
1711485677.693079: our call is friday so that would be after the deadline, do we need to setup a call before then?
1711485601.519429: <@U02EYQF4QH4> happy to discuss further on the call, but that’s what we need.
1711485574.710929: why is that? Are they going to lose access to the database? also <@U055K2ES0CW> I don't recall there being a specific deadline being discussed before for when the migration needed to be done.
1711485511.020719: we need metadata in the system by thursday.
1711485495.025829: That was the intent…but we are up against the clock
1711485465.697349: if you mean getting a dump of the postgres? Yes thats fine, but I thought the purpose of the access was so that we would run our CLI to extract the metadata on the live databases so we wouldnt have to transfer sql dumps
1711485419.827539: Or is there something specific you need to do?
1711485400.151719: If we extract it ourselves and send it to you would that be just as good?
1711485383.310849: No
1711485362.048929: Either way, can we get this DB extraction done today?
1711485335.700199: <@U055K2ES0CW> do you have access?
1711485286.718969: I haven't had time to test it yet, I'm still working on VDB updates to our CLI
1711485240.097279: <@U02EYQF4QH4> <@U055K2ES0CW> how we looking on LAC access?
1711414424.974959: Sounds good.  If you need me to host a working session tomorrow to “get you in” please let me know.  Happy to be the bridge if there are any delays on your access.
1711414185.417729: <@UMZBGNHAL>  We will get it in as soon as possible, checking with Carlos on timeline
1711414126.954069: <@U02EYQF4QH4> and <@U055K2ES0CW> please let me know what you need in order to complete your access to the LAC systems.  The customer is really pushing us to get this metadata loaded in ASAP.  If there are any other blockers or concerns that would further delay the process, please let me know.  Customer expect to see some custom metadata in the system by EOD Wednesday.
1711381365.750139: <@U02EYQF4QH4> and <@U055K2ES0CW> if you haven’t already, can you please reply to Tim ASAP at the LA Chargers to provide him your phone numbers so that he can setup your VPN access?  We are ready to get you plugged in to do the final DB extraction and begin the metadata import.
1711051996.187509: Alright, thanks. Yes, in other mams I've found that the limit on these dropdrowns has been tied to json payload limits, so I was curious if Iconik has worked around this in some way. I'll pose that to them directly, thanks!
1711051794.015569: I see no reason why there would be any issues, the only concern I would have is reaching some sort of max byte size but I think that limit is something that you might be better asking Iconik.
1711051663.697699: Perfect, thanks for the info! Glad to hear that search scales really well. On a related note, do you have any experience with how Iconik deals with very large dropdowns? With hundreds+ of options?
1711051323.511979: Note the performance issues with modifying large assets doesn’t affect anything other than they are queued and take some time to go through the queue but no performance issues in any other uses
1711051260.153949: Generally speaking Iconik does scale pretty well with a large number of assets the performance issues come in when trying to modify a large amount of assets at once as this will cause them to be queued and take some time. As far as for searching they use elastic search which scales really well but the re-indexing of a large number of files works well.
1711051245.689439: We've had clients with 9M+ that I didn't notice any performance issues on.
1711051117.403049: Do we know how Iconik scales with very large numbers of assets? Is there a hard/soft cap, and/or have you ever seen performance drops when dealing with a very large library?
1710537812.248679: <@UMZBGNHAL> I started the script, statuses are being set to online, this will take about as long as the set externa_id script since its about the same number of assets
1710442753.734249: <@U02EYQF4QH4> please respond after the meeting with IMT
1710442728.910299: So we can do one of two things:
1. I can remove all the archive scans that completed last night (probably by removing the _Archive - Reach Engine - Sources_ storage and then re-adding it) and then trigger a rescan once iconik implements their fix to the scanner.
2. I can keep the scans in place and then we can leverage a WIN-generated script to clean it all up.
1710442694.304179: well thats good
1710442627.496949: <@U055K2ES0CW> and <@U02EYQF4QH4> Mikael just responded with the following:
> We checked and it looks like an easy fix. I think we can fix it by Monday.
1710369603.601789: So I want to confirm, am I good to turn on the Archive scan for the Reach Engine Archive storage?
1710369601.643939: No problem, :grin:
1710369572.182509: <@U055K2ES0CW> I agree.  The assumption they made is not a good one.  We very much appreciate your willingness to step up and fix the online status for these assets (once the scan completes).
1710367719.225829: also, to be clear, iconik gave us the method to link these assets, but clearly did not state this would happen by doing so, them stating we HACKED it, is just weird since they are the one's who told us how to do it.
1710367660.300459: the script will need to be AFTER you perform the scan of the rest of the archive storage
1710367635.688089: We will take care of patching this, but to be clear, our code did not do this, this is iconik now supporting archived storage and having a silly method of evaluating the assets. Mikael stated that if you scan an archive storage the asset is marked as offline, this is a silly assumption iconik is making and they DO need to patch this. In the meantime, we are going to create a script to mark these assets as online
1710358312.117239: This is something for <@U055K2ES0CW> as I’m not exactly sure what is part of the SOW or not
1710358258.833319: I can do that…but can you provide a LOE and confirmation that WIN would cover the cleanup effort if we run the scan now?
1710358207.320969: I would say to press them for an actual timeline if this is possible and discuss with your team and the client if it fits with their timeline before doing anything. Just in my experience unless Iconik is already working on it they might take a while to implement and release it
1710358101.152439: So you’re suggesting I run the archive scan now?  If iconik is able to fix this quickly, then presumably this would be solved as part of the initial scan--and not after the fact.  If I run it now, then that would mean that we would need WIN to perform the cleanup after the fact.
1710353007.419669: Well in any case it seems the solution here might be to scan everything and then change the offline status for them all at once since you will need to run another scan 
1710352920.072899: Their solution is that someone needs to provide this manual scan &amp; cleanup.  I have requested that they bake this into the logic of the scanner, but per your point, we may need to just do this ourselves if its going to take them a long time to implement.
1710352869.211109: Here’s what we would be cleaning up:
• If an asset has an offline status set to TRUE and it has an original format located on the LAC_VIDEO ISG, then update/change the offline status to FALSE.
1710352763.784709: The question is what exactly are we cleaning up, I don’t understand what their solution was 
1710352500.593669: I believe so…sounds like they are suggesting “we” iterate through the assets and manually cleanup the online status values based on whatever criteria we define in the script.
1710349364.094759: <@UMZBGNHAL> got it. What’s strange is that Jason said we’ve done this in the past and this hasn’t happened but if they can push an update that would be great. In the meantime I wouldn’t holdout for Iconik to do this as they take forever to make updates. What are they suggesting? That we manually change the status to `Online` for these assets?
1710348604.417259: Per Mikhael’s point, I assume there is a way to subsequently crawl the assets to check and see if the asset has an online original and then update the asset status to “Online” but I tried to push back and see if they could just update their scanner to perform this validation.
1710348522.173499: TLDR, sounds like the external_id injection is confusing the iconik scanner when it attaches the S3 archived objects to an existing (online) asset.  For some reason, their scanner does not check to see if there is an online original associated with that asset record and instead immediately marks the asset as offline.
1710348451.908609: <@U02EYQF4QH4> and <@U055K2ES0CW> FYI:
1710284389.805489: <@U02EYQF4QH4> quick update:  I retested the archive scan and I’m still seeing the asset status appear offline post-scan.  I have sent over a message to the folks at Iconik, since I presume this is an iconik bug and nothing to do with the external_id script.  Below are a handful of screenshots that illustrate the problem:
1710199993.019409: Ok. I’ll try it tonight and take before/after screenshots. Is this an iconik bug or something we can address/fix after the fact through your migration script?
1709844510.025799: <@U055K2ES0CW> how we looking on LA Chargers Archive script?  Were you able to figure out how to inject the entire path into the External_ID or will it just be the filename?  We are working to resolve those duplicates before EOW, so we are pretty much ready to proceed with the scan whenever your script is ready.
1709672249.039869: Also <@UMZBGNHAL> we got a response from Iconik that they do not allow paths for external_id so we will run into this issue of potential duplicate names. I’m going to reach out to their team to make some further clarifications about how this will work so if you give me another day I can get us some more precise information on how Iconik handles these instances
1709672136.854809: Yes, and I believe we use other AWS services such as the AWS queueing service but it’s essential lambdas. 
1709671576.068849: ok great.  So is this essentially a Lambda that is deployed?
1709671557.028499: customer emvironment
1709671521.080499: <@U055K2ES0CW> <@U02EYQF4QH4> for your Auto-Scaler service/app, do you deploy the Autoscaler serverless function in the customers AWS environment or do you host that service?
1709332098.522539: <@UMZBGNHAL> here is the analysis for duplicate files. The total occurrence is 1,325 files.
1709317432.179339: 1.  Great!  Yep, we can review on our 1pm (PST) call.
2. Thanks.
1709317042.456499: 1. The script is ready to deploy and will only take a few minutes to deploy, but carlos wants to discuss with you today to make sure its all set correctly and then will deploy after the call. This would mean that the scan could begin on monday.
2. I am going to go ahead and ask <@U02EYQF4QH4> to clarify on this one
1709316958.320429: <@U055K2ES0CW> 2 things:
1. Just wanted to check in on the status of the new script + testing for being able to start the scan of the S3 Archive bucket for the LA Chargers?
2. On a related topic, when it comes to using iconik’s built-in Scan functionality on an S3 bucket, if I have no transcoder set during the initial ingest, but then I process a re-transcode, will it properly update the Media Type, etc.?  How about with camera cards?  Does iconik’s S3 scanner properly detect RED camera card folders (i.e. *.RDC). In my brief testing, the S3 scanner does not appear to honor the RDC card structure, whereas an ISG Scan does and bundles the contents into a single asset.
1708538617.256939: Hey <@U055K2ES0CW> any update on Carlos’s progress on the script for injecting a value into the _external_id_ for existing assets?
1707846444.218519: <@U055K2ES0CW> there would be a ton of duplicates if we go down this road, so it would be good to get clarity on this before we start any S3 archive scan.  Also, in this scenario, when would you be marrying up the archived objects with the iconik assets?
1707846249.371009: from dev : if that is the case then I have no clear way to prevent it, my guess would be to remove them later like we are doing with Orlando

i am also pushing to do some research further to see if we can prevent another orlando situation where we have to remove all the duplicate records
1707846109.915849: Thanks Nick. I believe Josh’s question/concern is around checksumming S3 Glacier (Deep Archive) objects and whether iconik is actually able to run checksums on those objects and then marry them up to the existing asset (if there was already a corresponding file on-prem)
1707846070.249149: from dev: it does need to do a checksum so it wont find them identical

 he (josh) is correct
1707845848.180809: based on my understanding, when iconik finds identical files, iconik registers them under the same asset. it doesn't matter what storage they are on if they are identical. This setting can be overridden if you want them to be separate assets, but i dont think that you do.

I sent your response back to dev as well
1707845481.511139: will that setting work even with these files in archive? Doesn't it need to run a checksum on them to compare?
1707845352.132639: <@U037S3RRK34> it shouldnt affect our CSV migration but you may want to go into scan settings and turn on turn on *Consider identical files the same*.
1707844702.005669: thank you!
1707844694.632279: <@U037S3RRK34> ill check with the team and get back to you before EOD,
1707844396.541919: <@U055K2ES0CW> question: We're almost done with the on-prem asset scan  at the chargers (should be done at EOD). For the archived files, do we need to import the CSV first, before turning on that scan? If we turn it on now we're afraid we're going to generate a lot of duplicate assets. Just want to confirm the correct order of operations for the CSV imports / scans.
1707844266.215329: <@U037S3RRK34> has joined the channel
1706050484.529139: Hey <@U055K2ES0CW> I just figured I would check-in and make sure you saw my email from 1/16.  We are about to pull the LA Chargers Reach Engine DB and send it over.  What is the best way and format to get that over to you?
1705001956.016269: thanks David!
1705001947.281819: look at that magic! (pun intended)
1705001830.375029: <@U035C70UDQ9> has joined the channel
1705001826.263109: Adding <@U035C70UDQ9>, our Slack workspace admin.
1704997912.942389: Copy that. Thanks!
1704997741.024499: <@UMZBGNHAL> This is what I found: ": The other organization will need to update the channel name on their end. This would typically require action from a Workspace Owner or Admin in the other organization." I think the workspace admin should be able to help.
1704997652.849259: nice
1704997382.953199: got it, i just read through the FAQ on this and they provide 0 guidance from a guest perspective from what i can see
1704997321.385649: Hmmm…alrighty then.  I’m still working with our workspace admin to see if this is actually something we need to update on our end.
1704997193.571959: unfortunately, no, im going to check now to see if i can fix it on my end beyond this
1704997102.000499: <@U055K2ES0CW> in regards to the channel name, can you see what happens if you grant us full rights to to the workspace?  Right now I see that we have “Can post &amp; invite” but I believe there is an access level above that.
1704936635.078149: Huh, strange looks completely different on our end.  Our channel is private…I think we need to talk to our Slack admin and have him update the channel name/info.
1704934899.959799: 
1704934840.664039: <@UMZBGNHAL> has joined the channel
1704934839.000199: <@UMZBGNHAL>
1702318687.530339: Hey IMT, just an update on Orlando Magic, we are processing the 5th set of 10,000 rows in the CSV import and progress is going smoothly as of now.
1700081584.721209: I will follow up with her now
1700081572.957839: yes I just wanted to make sure she hadn't forgotten the reply all button and something went direct to you
1700081098.380799: <@U5N6W046L>
This was in response to this email I sent on the 10th

Hey Jessica,

We actually need the files ingested first, then we can apply the metadata. (At least for the testing we are doing now).

I updated the scanner to only grab .mov and .mxf files. (Assuming you meant mxf and not mfx). However the ISG is not able to find files. I checked the user in Powershell with the same user as we are using in the iconik service and see issues around accessing files as well.

We are running this on the server at 10.160.140.56
1700081045.042309: I believe we are still waiting on Jessica.  This is the last email I saw from here: Apologies as our server has been down and I am not able to do this today. I plan to try again tomorrow.
1700071409.761609: <@U5N6W046L> Jason or carlos will get back to you about the vikings update, but for the sync, Jason will be out for the whole week, If a sync is needed for that week, we can at least provide a summary through Carlos and myself.
1700071123.840179: I assume you want to cancel their sync next week and not move it, yes?
1700071109.690739: <@U7CASSE6B> <@U02EYQF4QH4> any news on the vikings?
1699997034.649459: <@U7CASSE6B> have you heard anything for Jessica today?
1699899096.923259: <@U5N6W046L> Also, For Vikings, we ran into an issues with permissions on their files when we tried to configure the ISG for them. This needs to be resolved before the import of metadata can happen. Might be good to get on a call with them today if possible to show them and explain.
1699896117.043989: <@U7CASSE6B> <@U02EYQF4QH4> Good morning- looking for project status on Vikings and Magic.  Can you please let me know ASAP?
1699485277.432489: Not sure on the timing but if everything runs smoothly I can assume that by end of the week all the metadata should be in, The export today into CSV’s will only take 30min but the longer process is importing over 30 csv files with 10k entries each. Once we migrate the first 10k file we will have an idea for timing.
1699485044.782919: if you get anything for Vikings before our call please let me know- &amp; we have our sync tomorrow with the Magic and they are going to ask timing, do you have a rough idea?
1699485011.127159: I love that update for the Magic!  better late then never
1699484937.043079: As for Vikings I will have some new tomorrow as we are currently running a test of the new build that should improve performance and fix any errors we were previously getting.
1699484864.313449: <@U5N6W046L> Sorry for the late response but I wanted to update you that we did a test metadata migration of over 50 assets for Magic today and everything is working fine, we will preform one more final migration with the new settings of the entire database and we should be good to officially begin the migration of all the data tomorrow.
1699465347.600559: Morning!  Your favorite check in! :slightly_smiling_face: <@U7CASSE6B> <@U02EYQF4QH4> how we looking for the Vikings and Magic?
1699299477.083069: great news
1699299452.797869: <@U5N6W046L> I'm working on this all now. I have a complete export for both which we are doing testing on now. I'll get back to you as soon as I have the results
1699297811.952359: any news on the Magic?
1699297807.546159: <@U7CASSE6B> <@U02EYQF4QH4> <@U05B4QKRC1J> how we looking after the weekend for the Vikings?
1698940897.876379: <@U7CASSE6B> lets bring it up with them today on the cadence.  Any news this morning for the VIkings?
1698880253.739189: <@U5N6W046L> Yes, that would be good. In the meantime I am checking to see if we can optimize the code a bit more to help with this as well
1698872073.702219: do you want to ping them on the thread and ask?
1698871986.395019: Not sure to be honest but we should bring up the server issues to see if maybe there was some interruption or log on their end. It’s strange as the process ran for more than 24hrs and made thousands and thousands of the same API call but suddenly stopped working, and the system attempted 5 more times after waiting a period between each attempt.
1698871865.369349: ok do you think you will have that information before our sync tomorrow so we can bring it up with the client?
1698871709.978589: <@U5N6W046L> We were able to get more than halfway through the metadata extraction but the exporting stopped due to getting a 500 error code back from the Server using the Portal API, we have our devs looking into it and hopefully we can continue the process soon. If they encounter this issue again we might have to look at the server as to why this happened as a 500 response is a server issue. 
1698871464.421319: how are we looking for the Vikings? <@U02EYQF4QH4> <@U7CASSE6B>
1698856315.731899: <@U7CASSE6B> <@U02EYQF4QH4> are you joining this Magic call?
1698779688.512949: You can let them know, but we will keep the thread updated as well!
1698779150.465199: thats great news, can I let the customer know or would you be doing so on the thread?
1698777875.377889: <@U5N6W046L> the Vikings Metadata export is about half way through and we will hopefully have it complete by end of day today.
1698775221.501279: checking with Carlos <@U5N6W046L>
1698774752.158719: how did the Vikings look today <@U02EYQF4QH4>? can we get Jessica an update?
1698770794.202259: thank you!
1698770656.055169: yes that works in Jasons calendar!
1698769078.956279: <@U02EYQF4QH4> <@U7CASSE6B> just want to confirm you are ok pushing the Magic sync by 30 tomorrow?
1698707931.727209: Understood, thx 
1698707689.475479: I will have something tomorrow by end of day.
1698707238.715149: Ok thanks for the update. For the Magic- when do you anticipate an update? The client is getting anxious so I want to make sure we have this information available 
1698706250.998029: <@U5N6W046L> Sorry for the late response, but I have updates. Vikings Export crashed due to some OS compatibility issues but these should have been fully fixed and we are now attempting a new EXPORT that will take several hours so we will have an update on that tomorrow. As for Orlando Magic we have identified that the exported data is using the Reach Engine Friendly name that contained spaces and not the file paths exact name. We are updating the tool to use the new file path name. Once a successful import of the same sample data as before is working we will move on to the larger data set and update you on the progress.
1698684949.366809: thanks Jason I appreciate it
1698684924.215649: Hey <@U5N6W046L>. <@U02EYQF4QH4> Is working on it now. He should be able to send over an update soon. I am back in the office today so I will get back into everything and ensure we get some more solid specific updates over to you by end of day today or tomorrow with a specific idea on timing to completion for both.
1698681739.689669: morning!  <@U02EYQF4QH4> <@U05B4QKRC1J> checking in on Magic and Vikings- any news?  Customer is asking this morning via email specifically for the Vikings
1698428708.048779: Not at the moment. I will update you once I get a more clear timeline. 
1698425068.436759: do we have ETAs on either that you are targeting at?
1698424919.486729: We are still looking into the Naming issue on Magic before continuing with the Migration. 
1698424858.005469: <@U5N6W046L> We were able to complete the install on the Viking server but we ran into some technical issues, I have yet to send an update as I am waiting for a response from our dev team to see what is the cause. 
1698424780.370499: But Carlos is going to address those points
1698424769.906829: I am out sick today 
1698424765.615819: Hello Amanda 
1698423433.739389: and were you able to complete the install at the Vikings? <@U05B4QKRC1J>
1698423409.674389: <@U05B4QKRC1J> any news on the Magic?
1698352750.179699: We can all jump on my link <@U02EYQF4QH4> <@U5N6W046L>
1698352740.260239: <https://us05web.zoom.us/j/85959834267?pwd=PlJurP3rYKYEuMEKbSYpPvD2cqUFjz.1>
1698352715.770279: can you pass an update to the client still via email?
1698352675.907009: <@U5N6W046L>
1698352672.927849: Yes can you reschedule for tomorrow or preferably Monday?
1698352620.939119: someone is using my link otherwise i would just use mine
1698352600.420069: do we need to reschedule or can you spin up something else?
1698352497.784069: ok
1698352460.623049: We are trying to start the call as Jason is out and Angel our VA will start the call in 2 min <@U5N6W046L>
1697739677.318629: hi <@U055K2ES0CW> <@U7CASSE6B> can you please share a recording of this meeting with IMT when we wrap this?
1697579548.688249: Will do. Thanks. I'm sending over an email to Matt and Jessica now with this error (you will be cc'd).
1697579444.761539: thanks Jason, if we need to have a conversation with them let me know
1697579360.711989: I will try and get into their system to get some more specifics later today and send over an email with more details as I find them .
1697579313.020749: Having folder names with spaces at the end will break most software processes.
1697579291.113269: Hey Amanda. I setup a scan of their content. The performance is looking good, but it looks like we may need to do some cleanup of their folder structure. I'm seeing a lot of errors in iconik like this one: C:\Isilon\VEN\2020\Project Success \EXPORTS:Folder "Project Success " has trailing spaces in the name. This can stop ingest from progressing! Please, remove the trailing spaces from the folder name
1697579110.759469: <@U7CASSE6B> <@U05B4QKRC1J> wanted to check in and see how everything is going at the vikings? did the increase help?
1695936862.271749: thanks Nick
1695936500.247869: checking now
1695935932.102799: <@U7CASSE6B> <@U055K2ES0CW> the customer (Magic) approved the auto scaler quote and you should have the PO in your inbox soon.  Is there anything we can do to get this up and running before 10/2?  The Magic need it for Monday.
1695829758.506439: No worries
1695829683.884429: <@U7CASSE6B> <@U05B4QKRC1J> just a heads up that I will be a few minutes late to the magic sync today 
1695233935.280689: ok I will call that out, thank you
1695233782.669029: Yes please followup. It would be best to do during an evening Monday - Thursday if possible.
1695233734.308159: <@U7CASSE6B> I assume you haven't pulled a copy of the database for the vikings yet?  Do you need me to follow up?  Do you have a preferred day/time for it?
1694808412.883559: or <@U05B4QKRC1J> on his behalf?
1694808353.565869: <@U7CASSE6B> are you able to respond to Jessica?  I thought I responded to her earlier that we aren't ready to do this yet?
1694807654.873089: is there a different price to IMT vs price to customer? otherwise are we clear to mark it up?
1694807579.474549: <@U4GBQ7YE6> share this with any of your clients you would like!
1694806901.336459: <@U055K2ES0CW> that works for me, just a quote for 1 year including that setup would be what we need
1694806314.347119: Hi Roy! as far as this SOW, I may have missed where you were talking about this previously, are you referring to the Autoscaler Jason posted about above? This will not require a full SOW but runs at a one time initial setup fee of $2,500.00 and a monthly subscription of $1197.00 a month for access to the utility. This is a monthly contract and can be cancelled at anytime before the start of the next month. The setup fee is not necessary after the initial setup. Our goal here is to make it as easy as possible for companies to only pay for the utility when they need it!

I am happy to send over an estimate with this information at your convenience, but if this isn't what you were looking for, I will find out when I speak to Jason later this afternoon :smiley:
1694802302.191269: we have at least one other customer likely to bite on this same thing
1694802286.641289: <@U7CASSE6B> can you provide an SOW that would include the cost of the connector and hours needed to configure/deploy?
1694800344.218189: Thank you! 
1694800250.155999: will do
1694799953.684309: can we add Melstrom, Josh &lt;<mailto:MelstromJ@Vikings.nfl.net|MelstromJ@Vikings.nfl.net> to the vikings sync moving forward too please?
1694799656.490359: and then let you know
1694799652.942339: ill check with the sales team if they need anything additional
1694799632.150009: Let me know if you want me to send direct, or you can adjust if needed
1694799625.169709: oh nice, thank you!
1694799610.670519: This has our monthly price in it
1694799601.448189: <@U5N6W046L>
1694723557.215769: <@U7CASSE6B> <@U05B4QKRC1J> im waiting to get on the Vikings sync, do you have a call running long?
1694709004.016209: thank you!
1694708961.315609: noted <@U5N6W046L>
1694706997.734459: <@U02DNUEU71D> can you please add Matt Freshwater to the Vikings invite moving forward?
1694114424.314039: Ok. Will do. <@U02DNUEU71D> :eyes:
1694114370.890289: I forwarded it to him but I’m not the owner 
1694112354.309059: <@U7CASSE6B> do you want to forward Matt the invite so he can join if you haven’t already? I piggybacked on your email but let’s just hit him 
1694107314.812169: haha jk
1694107313.353909: i don't
1694107306.488199: us too
1694107296.088349: Yes, I did. Hoping maybe they have an update today... :wink:
1694107270.642029: I do believe Jason asked that last night on the thread, yes?
1694107235.033249: Hello <@U5N6W046L> Can we make sure the individual in control of giving us server access with the jump server is able to join the meeting so we can past the wall block for Vikings.
1694022995.683809: We still need to test them standby once test is validate can you comment here <@U7CASSE6B>
1693931050.435439: I dont think I have seen this come through.
1693930862.587419: <@U05B4QKRC1J> <@U7CASSE6B> did you get the creds from Jessica for the Cantemo Portal? I know she was sending it through a secure method.
1693418556.621089: all good, I just want to make sure I am getting you what you need so if you see Im not on add me so I can do my follow ups for ya :slightly_smiling_face:
1693416933.120999: Yeah, sorry about that. That email thread was started by Alex.
1693413245.636469: ok , just make sure to keep me added so i can follow up
1693412413.186949: That thread was created I am assuming from when Jason needed live support on VPN password and I think that how it was started.
1693411214.059949: is there a reason I wasn't copied on that original correspondence? If there is an issue I should be made aware
1693411145.671459: he usually does
1693410837.858289: I see his is listed as a invitee
1693410815.336959: <@U5N6W046L> Can you ping Alex and see if he can join todays sync?
1693354165.680299: Sure. That would be great
1693353934.834499: I can do that if you would like me to 
1693353916.341029: We can ask Jessica again, she was who looped in Matt for access 
1693353400.157109: <@U0152BJS866> has joined the channel
1693353392.010629: Please let me know if there is anyone else I should email about getting the access to the actual Cantemo server now that VPN is all working
1693353346.215559: <@U5N6W046L> I just sent an email in regards to Vikings. I also saw that Matt mentioned "After a successful VPN connection, you should be able to work with our VEN team and access Cantemo."
1692669166.343269: awesome, thanks.  I am in
1692669065.467949: <@U5N6W046L> try now
1692669009.339429: <@U7CASSE6B> I dont have access to this board, can you please share?
1692664642.353309: <@U5N6W046L> here you go: <https://miro.com/app/board/uXjVMf_Tqgs=/?moveToWidget=3458764561714917554&amp;cot=14|https://miro.com/app/board/uXjVMf_Tqgs=/?moveToWidget=3458764561714917554&amp;cot=14>
1692664555.685489: Sure. It’s on the Miro. I’ll grab the link and post it here
1692662114.864669: <@U7CASSE6B> not sure if you saw my email but zoom stopped working on me so apologies for dropping- can you send me the list you were going over so we can have it ready for the customer sync call tomorrow?
1692646746.999569: <@U7BG52ZAM> has joined the channel
1692646743.620379: Adding <@U7BG52ZAM> as well for help with the Magic
1692209577.125349: Project Status Update: Vikings

Date: 8/16/23


1. VPN Access to Vikings Environment:
    ◦ Status: Pending.
    ◦ Details: WIN currently lacks access to the VPN Vikings environment, which is critical for seamless project integration and development tasks.
    ◦ Action Required: Please provide the necessary credentials, instructions, and any additional information required to access the Vikings environment via VPN.
2. WIN’s Environment Setup:
    ◦ Status: In Progress.
    ◦ Details: WIN is in the process of configuring its own environment for the Vikings project.
    ◦ Next Steps: Once the environment setup is complete and tested, WIN will begin integration tasks contingent on VPN access to the Vikings environment.
Request:
For timely and efficient progression on the Vikings project, we request expedited access to the VPN Vikings environment. Early access will ensure that there are no unnecessary delays in setting up, testing, and deploying the required solutions.
1692209201.126459: We can review this together as well
1692209183.926689: 
1692209145.762179: Development Status Report
<@U5N6W046L>

Date: 8/16/2023


Project: Orlando Magic with IMT

1. Initial Scan Setup for Iconik via ISG:
• Current Status: Awaiting feedback from IMT regarding server configurations, proxy generation criteria, and access to the Orlando Magic iconik environment.
    ◦ Next Steps:Start the necessary configurations upon receiving information.
    ◦ Conduct an initial scan test.
    ◦ Provide feedback or highlight any challenges for swift resolution.
2. Development Updates:
    ◦ Support Migrated Collections:Current Status: QA (Quality Assurance) testing.
    ◦ Challenges: [If any, specify here.]
    ◦ Next Steps: Once QA is passed, move to the next phase, which might be deployment or user acceptance testing.
Notes:
• Regular updates and feedback will be provided to ensure all stakeholders are informed of progress.
• Any potential roadblocks or challenges will be communicated immediately.
1691774303.010779: has renamed the channel from "imt-win-shared" to "imt-win-workspace"
1691773837.505089: awesome, thanks!
1691773826.693229: <@U5N6W046L>
1691773821.858509: Yes of course Amanda Ill get that done
1691773790.141849: <@U05B4QKRC1J> any luck getting this renamed to just IMT/WIN workspace?
1691087436.249919: <@U055K2ES0CW> has joined the channel
1691086000.169609: has renamed the channel from "orlando-magic-win-shared" to "imt-win-shared"
1690830375.243859: Hello Welcome team to our shared channel <!channel>
1690824300.700179: <@U9CQMQXHQ> has joined the channel
1690824295.870119: <@U5N6W046L> has joined the channel
1690824291.707609: <@URXM7FUF3> has joined the channel
1690824286.483749: <@UJ1N2C67Q> has joined the channel
1690824257.605869: <@U4GBQ7YE6> has joined the channel
1690824257.514889: *IMT Global, Inc.* has joined this channel by invitation from *Workflow Intelligence Nexus*.
1690824121.846349: <@U02EYQF4QH4> has joined the channel
1690824121.771809: <@U7CASSE6B> has joined the channel
1690824121.705799: <@U05B4QKRC1J> has joined the channel
1690824074.707619: <@U02DNUEU71D> has joined the channel
